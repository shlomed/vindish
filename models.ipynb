{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "PRINT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c_dir = \"C://Users/shlomi/Documents/Work/vindish/data/\"  \n",
    "e_dir = \"E:\\\\Work/Vindish/created_samples/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = torch.tensor(np.load(e_dir + \"X.npy\")).type(torch.float32)  \n",
    "y = torch.tensor(np.load(e_dir + \"y.npy\")).type(torch.float32)  \n",
    "features = np.load(e_dir+\"features.npy\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### subtruct 1 so the dom will be in the [0,30] range for embeddings\n",
    "X[:,:,8] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniConv1d(torch.nn.Module):\n",
    "    def __init__(self, init_kernel_size=(3, 2)):\n",
    "        super(MiniConv1d, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 5, init_kernel_size, padding=(1, 1))\n",
    "        self.conv2 = torch.nn.Conv2d(5, 5, (3, 3), padding=(1, 1))\n",
    "        self.conv3 = torch.nn.Conv2d(5, 5, (3, 3), padding=(1, 1))\n",
    "        self.conv4 = torch.nn.Conv2d(5, 5, (3, 3), padding=(1, 1))\n",
    "        self.conv5 = torch.nn.Conv2d(5, 5, (3, 3), padding=(1, 1))\n",
    "        self.conv6 = torch.nn.Conv2d(5, 5, (3, 3), padding=(1, 1))\n",
    "        self.conv7 = torch.nn.Conv2d(5, 5, (3, 3), padding=(1, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv7(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniConv2d(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MiniConv2d, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 5, 3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(5, 5, 3, padding=1)\n",
    "        self.conv3 = torch.nn.Conv2d(5, 5, 3, padding=1)\n",
    "        self.conv4 = torch.nn.Conv2d(5, 5, 3, padding=1)\n",
    "        self.conv5 = torch.nn.Conv2d(5, 5, 3, padding=1)\n",
    "        self.conv6 = torch.nn.Conv2d(5, 5, 3, padding=1)\n",
    "        self.conv7 = torch.nn.Conv2d(5, 5, 3, padding=1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv7(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mini_conv2d = MiniConv2d()\n",
    "mini_conv2d(X[:3, :5].unsqueeze_(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(torch.nn.Module):\n",
    "    def __init__(self, n_categories, n_dims):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(n_categories, n_dims)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.mini_conv_ux = MiniConv2d()\n",
    "        self.mini_conv_ux_diffs = MiniConv2d()\n",
    "        self.mini_conv_snp = MiniConv1d()\n",
    "        self.mini_conv_singles = MiniConv1d(init_kernel_size=(3,3))\n",
    "        self.embed_dow = Embeddings(7, 3)\n",
    "        self.embed_dom = Embeddings(32, 5)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(880, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 31)\n",
    "        self.fc3 = torch.nn.Linear(31, 5)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ux_vals = x[:, :, 1:6].unsqueeze_(1)\n",
    "        ux_diffs = x[:, :, 10:15].unsqueeze_(1)\n",
    "        snp_data = x[:, :, [6,15]].unsqueeze_(1)\n",
    "        singles = x[:, :,  [0, 18, 19]].unsqueeze_(1) # time_to_expiration, doy, time_of_day\n",
    "        \n",
    "        dow = x[:, :, 17].type(torch.long)\n",
    "        dom = x[:, :, 16].type(torch.long)\n",
    "        \n",
    "        x_ux = self.mini_conv_ux(ux_vals).view(x.shape[0], -1)\n",
    "        x_diffs = self.mini_conv_ux_diffs(ux_diffs).view(x.shape[0], -1)\n",
    "        x_snp = self.mini_conv_snp(snp_data).view(x.shape[0], -1)\n",
    "        x_dow = self.embed_dow(dow).view(x.shape[0], -1)\n",
    "        x_dom = self.embed_dom(dom).view(x.shape[0], -1)\n",
    "        x_singles = self.mini_conv_singles(singles).view(x.shape[0], -1)\n",
    "        \n",
    "        x = torch.cat((x_ux, x_diffs, x_snp, x_dom, x_dow, x_singles), 1)\n",
    "        \n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profit(y, x, alphas):\n",
    "    if PRINT:\n",
    "        print(\"Xs:\\n\", x[0].cpu())\n",
    "        print(\"ys:\\n\", y[0].cpu())\n",
    "\n",
    "    L3 = ((y - x)*alphas).sum(dim=1)\n",
    "\n",
    "    if PRINT:\n",
    "        print(\"L3s:\\n\", L3[:2].cpu())\n",
    "    return L3\n",
    "\n",
    "def get_dist_from_200(alphas):\n",
    "    return (alphas.abs().sum(dim=1)-200.)\n",
    "\n",
    "def get_hedging_score(alphas, betas):\n",
    "    return (alphas*betas).sum(dim=1)\n",
    "\n",
    "def calc_loss(alphas, betas, x_batch, y_batch):\n",
    "    a = 1\n",
    "    b = 10\n",
    "    c = 10000\n",
    "    \n",
    "    L1 = get_dist_from_200(alphas)**2\n",
    "    L2 = get_hedging_score(alphas, betas)**2 \n",
    "    L3 = get_profit(y_batch, x_batch[:, -1, 1:6], alphas)\n",
    "    L = a*L1 + b*L2 - c*L3\n",
    "    \n",
    "#     print(L1.size(), L2.size(), L3.size())\n",
    "    \n",
    "    return L.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1",
   "language": "python",
   "name": "pytorch1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
