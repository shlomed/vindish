{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shlomi\\AppData\\Local\\conda\\conda\\envs\\rl1\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import add_features\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "PRINT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_dir = \"C://Users/shlomi/Documents/Work/vindish/data/\"\n",
    "e_dir = \"E:\\\\Work/Vindish/created_samples/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(np.load(e_dir + \"X.npy\")).type(torch.float32)\n",
    "y = torch.tensor(np.load(e_dir + \"y.npy\")).type(torch.float32)\n",
    "features = np.load(e_dir+\"features.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49938, 10, 20]) torch.Size([49938, 5])\n",
      "torch.FloatTensor torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)\n",
    "print(X.type(), y.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Time_To_Expiration\n",
      "1 UX1\n",
      "2 UX2\n",
      "3 UX3\n",
      "4 UX4\n",
      "5 UX5\n",
      "6 SP500\n",
      "7 dow\n",
      "8 dom\n",
      "9 doy\n",
      "10 UX1_diff\n",
      "11 UX2_diff\n",
      "12 UX3_diff\n",
      "13 UX4_diff\n",
      "14 UX5_diff\n",
      "15 SP500_diff\n",
      "16 day_of_month\n",
      "17 day_of_week\n",
      "18 day_of_year\n",
      "19 time_of_day\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(features):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtruct 1 so the dom will be in the [0,30] range for embeddings\n",
    "X[:,:,8] -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniConv2d(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MiniConv2d, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 5, 3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(5, 5, 3, padding=1)\n",
    "        self.conv3 = torch.nn.Conv2d(5, 5, 3, padding=1)\n",
    "        self.conv4 = torch.nn.Conv2d(5, 5, 3, padding=1)\n",
    "        self.conv5 = torch.nn.Conv2d(5, 5, 3, padding=1)\n",
    "        self.conv6 = torch.nn.Conv2d(5, 5, 3, padding=1)\n",
    "        self.conv7 = torch.nn.Conv2d(5, 5, 3, padding=1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv7(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mini_conv2d = MiniConv2d()\n",
    "mini_conv2d(X[:3, :5].unsqueeze_(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniConv1d(torch.nn.Module):\n",
    "    def __init__(self, init_kernel_size=(3, 2)):\n",
    "        super(MiniConv1d, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 5, init_kernel_size, padding=(1, 1))\n",
    "        self.conv2 = torch.nn.Conv2d(5, 5, (3, 3), padding=(1, 1))\n",
    "        self.conv3 = torch.nn.Conv2d(5, 5, (3, 3), padding=(1, 1))\n",
    "        self.conv4 = torch.nn.Conv2d(5, 5, (3, 3), padding=(1, 1))\n",
    "        self.conv5 = torch.nn.Conv2d(5, 5, (3, 3), padding=(1, 1))\n",
    "        self.conv6 = torch.nn.Conv2d(5, 5, (3, 3), padding=(1, 1))\n",
    "        self.conv7 = torch.nn.Conv2d(5, 5, (3, 3), padding=(1, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv7(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mini_conv1d = MiniConv1d()\n",
    "mini_conv1d(X[:3, :5].unsqueeze_(1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mini_conv_singles = MiniConv1d(init_kernel_size=(3,3))\n",
    "singles = X[:, :,  [0, 18, 19]].unsqueeze_(1)\n",
    "mini_conv_singles(singles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(torch.nn.Module):\n",
    "    def __init__(self, n_categories, n_dims):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(n_categories, n_dims)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "embed = torch.nn.Embedding(6, 2)\n",
    "embed(torch.tensor([[1,1,3,4], [1,1,3,4]])).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "embed = Embeddings(7, 2)\n",
    "embed(X[:, :, 7].type(torch.long)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.mini_conv_ux = MiniConv2d()\n",
    "        self.mini_conv_ux_diffs = MiniConv2d()\n",
    "        self.mini_conv_snp = MiniConv1d()\n",
    "        self.mini_conv_singles = MiniConv1d(init_kernel_size=(3,3))\n",
    "        self.embed_dow = Embeddings(7, 3)\n",
    "        self.embed_dom = Embeddings(31, 5)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(880, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 5)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ux_vals = x[:, :, 1:6].unsqueeze_(1)\n",
    "        ux_diffs = x[:, :, 10:15].unsqueeze_(1)\n",
    "        snp_data = x[:, :, [6,15]].unsqueeze_(1)\n",
    "        singles = x[:, :,  [0, 18, 19]].unsqueeze_(1) # time_to_expiration, doy, time_of_day\n",
    "        \n",
    "        dow = x[:, :, 17].type(torch.long)\n",
    "        dom = x[:, :, 16].type(torch.long)\n",
    "        \n",
    "        x_ux = self.mini_conv_ux(ux_vals).view(x.shape[0], -1)\n",
    "        x_diffs = self.mini_conv_ux_diffs(ux_diffs).view(x.shape[0], -1)\n",
    "        x_snp = self.mini_conv_snp(snp_data).view(x.shape[0], -1)\n",
    "        x_dow = self.embed_dow(dow).view(x.shape[0], -1)\n",
    "        x_dom = self.embed_dom(dom).view(x.shape[0], -1)\n",
    "        x_singles = self.mini_conv_singles(singles).view(x.shape[0], -1)\n",
    "        \n",
    "        x = torch.cat((x_ux, x_diffs, x_snp, x_dom, x_dow, x_singles), 1)\n",
    "        \n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "betas = torch.from_numpy(np.array([0.62, 0.44, 0.32, 0.26, 0.21])).type(torch.Tensor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(X.shape[0]*0.6)\n",
    "n_val = int(X.shape[0]*0.8)\n",
    "\n",
    "X_train = X[:n_train]\n",
    "y_train = y[:n_train]\n",
    "\n",
    "X_val = X[n_train:n_val]\n",
    "y_val = y[n_train:n_val]\n",
    "\n",
    "# dropping all end of period samples:\n",
    "ser = pd.Series(X_train[:, -1, 0].detach().numpy())\n",
    "idx_to_keep = ser[ser>0.3].index.values\n",
    "X_train = X_train[idx_to_keep]\n",
    "y_train = y_train[idx_to_keep]\n",
    "\n",
    "ser = pd.Series(X_val[:, -1, 0].detach().numpy())\n",
    "idx_to_keep = ser[ser>0.3].index.values\n",
    "X_val = X_val[idx_to_keep]\n",
    "y_val = y_val[idx_to_keep]\n",
    "\n",
    "\n",
    "X_test = X[n_val:]\n",
    "y_test = y[n_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(X_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_ds = TensorDataset(X_val, y_val)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "# print(model)\n",
    "# model.fc3.weight.data = (torch.zeros_like(model.fc3.weight, requires_grad=True, device=device))\n",
    "# model.fc3.bias.data = torch.tensor([200., 0., 0., 0., 0.], requires_grad=True, device=device)\n",
    "\n",
    "model = model.to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.forward(X_train[:BATCH_SIZE]).shape, X_train[:BATCH_SIZE].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in model.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profit(y, x, alphas):\n",
    "    if PRINT:\n",
    "        print(\"Xs:\\n\", x[0].cpu())\n",
    "        print(\"ys:\\n\", y[0].cpu())\n",
    "\n",
    "    L3 = ((y - x)*alphas).sum(dim=1)\n",
    "\n",
    "    if PRINT:\n",
    "        print(\"L3s:\\n\", L3[:2].cpu())\n",
    "    return L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_from_200(alphas):\n",
    "    return (alphas.abs().sum(dim=1)-200.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hedging_score(alphas, betas):\n",
    "    return (alphas*betas).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(alphas, betas, x_batch, y_batch):\n",
    "    a = 1\n",
    "    b = 10\n",
    "    c = 10000\n",
    "    \n",
    "    L1 = get_dist_from_200(alphas)**2\n",
    "    L2 = get_hedging_score(alphas, betas)**2 \n",
    "    L3 = get_profit(y_batch, x_batch[:, -1, 1:6], alphas)\n",
    "    L = a*L1 + b*L2 - c*L3\n",
    "    \n",
    "#     print(L1.size(), L2.size(), L3.size())\n",
    "    \n",
    "    return L.sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x.dtype, y.dtype, betas.dtype, alphas.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2472548.2\n",
      "688502.9\n",
      "13228.117\n",
      "59236.375\n",
      "9619.929\n",
      "epoch = 0; mean_epoch_loss = 699403.688; val_loss = 9619.929; dist_from_200_loss = 6.371; betas_loss = -3.688; mean_buy_sell_expenses = 0.377; mean_profit = 0.005307; mean_profit_with_expenses = 0.004930;\n",
      "saving model.\n",
      "-29550.352\n",
      "-15486.045\n",
      "57683.906\n",
      "85237.33\n",
      "2892.402\n",
      "epoch = 1; mean_epoch_loss = 16501.129; val_loss = 2892.402; dist_from_200_loss = 4.531; betas_loss = 0.527; mean_buy_sell_expenses = 0.366; mean_profit = -0.000743; mean_profit_with_expenses = -0.001108;\n",
      "saving model.\n",
      "9724.229\n",
      "20110.082\n",
      "62988.613\n",
      "-23021.287\n",
      "6477.698\n",
      "epoch = 2; mean_epoch_loss = 16080.700; val_loss = 6477.698; dist_from_200_loss = 9.049; betas_loss = 0.127; mean_buy_sell_expenses = 0.368; mean_profit = -0.000460; mean_profit_with_expenses = -0.000828;\n",
      "not saving model.\n",
      "1522.8654\n",
      "53843.97\n",
      "8038.478\n",
      "-17307.988\n",
      "4067.2468\n",
      "epoch = 3; mean_epoch_loss = 13238.548; val_loss = 4067.247; dist_from_200_loss = 7.094; betas_loss = -0.667; mean_buy_sell_expenses = 0.362; mean_profit = 0.000522; mean_profit_with_expenses = 0.000160;\n",
      "not saving model.\n",
      "47208.574\n",
      "-23140.535\n",
      "-21321.947\n",
      "34908.17\n",
      "7324.7983\n",
      "epoch = 4; mean_epoch_loss = 15666.450; val_loss = 7324.798; dist_from_200_loss = 9.902; betas_loss = -0.171; mean_buy_sell_expenses = 0.365; mean_profit = -0.000332; mean_profit_with_expenses = -0.000697;\n",
      "not saving model.\n",
      "2992.96\n",
      "7294.4263\n",
      "-66436.41\n",
      "82264.87\n",
      "5424.3496\n",
      "epoch = 5; mean_epoch_loss = 13762.404; val_loss = 5424.350; dist_from_200_loss = 7.093; betas_loss = -2.106; mean_buy_sell_expenses = 0.357; mean_profit = 0.002359; mean_profit_with_expenses = 0.002002;\n",
      "not saving model.\n",
      "-42419.13\n",
      "60869.66\n",
      "45620.254\n",
      "21990.877\n",
      "32988.91\n",
      "epoch = 6; mean_epoch_loss = 13822.886; val_loss = 32988.910; dist_from_200_loss = 21.185; betas_loss = 1.535; mean_buy_sell_expenses = 0.374; mean_profit = -0.003185; mean_profit_with_expenses = -0.003559;\n",
      "not saving model.\n",
      "28830.797\n",
      "-4463.524\n",
      "30964.637\n",
      "25040.945\n",
      "11087.075\n",
      "epoch = 7; mean_epoch_loss = 13052.941; val_loss = 11087.075; dist_from_200_loss = 9.789; betas_loss = 1.766; mean_buy_sell_expenses = 0.356; mean_profit = -0.003574; mean_profit_with_expenses = -0.003930;\n",
      "Epoch     7: reducing learning rate of group 0 to 3.3000e-05.\n",
      "not saving model.\n",
      "-6633.6006\n",
      "-5992.5225\n",
      "3032.83\n",
      "-16493.336\n",
      "3525.9912\n",
      "epoch = 8; mean_epoch_loss = 9957.662; val_loss = 3525.991; dist_from_200_loss = 6.169; betas_loss = -0.858; mean_buy_sell_expenses = 0.349; mean_profit = 0.000209; mean_profit_with_expenses = -0.000141;\n",
      "not saving model.\n",
      "-17541.135\n",
      "25813.887\n",
      "-2186.8374\n",
      "47369.81\n",
      "3927.421\n",
      "epoch = 9; mean_epoch_loss = 10512.728; val_loss = 3927.421; dist_from_200_loss = 6.478; betas_loss = -0.252; mean_buy_sell_expenses = 0.349; mean_profit = -0.000711; mean_profit_with_expenses = -0.001059;\n",
      "not saving model.\n",
      "29167.95\n",
      "-18100.633\n",
      "2288.6697\n",
      "-16398.096\n",
      "4339.0273\n",
      "epoch = 10; mean_epoch_loss = 9682.413; val_loss = 4339.027; dist_from_200_loss = -0.096; betas_loss = 1.564; mean_buy_sell_expenses = 0.339; mean_profit = -0.003361; mean_profit_with_expenses = -0.003700;\n",
      "not saving model.\n",
      "63967.63\n",
      "27950.443\n",
      "31975.963\n",
      "-60264.22\n",
      "7270.092\n",
      "epoch = 11; mean_epoch_loss = 10152.971; val_loss = 7270.092; dist_from_200_loss = 9.329; betas_loss = 0.257; mean_buy_sell_expenses = 0.350; mean_profit = -0.001563; mean_profit_with_expenses = -0.001913;\n",
      "not saving model.\n",
      "-38364.344\n",
      "54179.16\n",
      "19963.883\n",
      "-43206.54\n",
      "1181.2993\n",
      "epoch = 12; mean_epoch_loss = 9437.134; val_loss = 1181.299; dist_from_200_loss = 1.226; betas_loss = -0.729; mean_buy_sell_expenses = 0.337; mean_profit = -0.000121; mean_profit_with_expenses = -0.000459;\n",
      "saving model.\n",
      "48495.266\n",
      "-5798.925\n",
      "-8359.142\n",
      "33753.39\n",
      "10694.946\n",
      "epoch = 13; mean_epoch_loss = 9650.729; val_loss = 10694.946; dist_from_200_loss = 12.133; betas_loss = -0.301; mean_buy_sell_expenses = 0.350; mean_profit = -0.000865; mean_profit_with_expenses = -0.001215;\n",
      "not saving model.\n",
      "29847.557\n",
      "1573.1014\n",
      "-39378.15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-fcd934a402cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\rl1\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\rl1\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses_epoch = []\n",
    "losses_val_history = []\n",
    "mean_buy_sell_expenses = []\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.33, verbose=True)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for i, (x, y) in enumerate(train_dl):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "#         print(\"Xs:\\n\", x[:2, -1, :].cpu())\n",
    "#         print(\"ys:\\n\", y[:2, :].cpu())\n",
    "#         print(x)\n",
    "        alphas = model(x)#.type(torch.Tensor)#.to(device)        \n",
    "        loss = calc_loss(alphas, betas, x, y)\n",
    "        losses_epoch.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%100==99:\n",
    "            print(np.mean(losses_epoch[-100:]))\n",
    "            \n",
    "    model.eval()\n",
    "    losses_val = []\n",
    "    L1s = []\n",
    "    L2s = []\n",
    "    profits = []\n",
    "    for x_val, y_val in val_dl:\n",
    "        x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "        alphas_val = model(x_val).type(torch.Tensor).to(device)        \n",
    "        loss_val = calc_loss(alphas_val, betas, x_val, y_val)\n",
    "        losses_val.append(loss_val.cpu().detach().numpy())\n",
    "        L1s.append(get_dist_from_200(alphas_val).cpu().detach().numpy().mean())\n",
    "        L2s.append(get_hedging_score(alphas_val, betas).cpu().detach().numpy().mean())\n",
    "        profits.append(get_profit(y_val, x_val[:, -1, 1:6], alphas_val).cpu().detach().numpy().mean())\n",
    "        mean_buy_sell_expenses.append(pd.DataFrame(alphas_val.detach().cpu().numpy()).diff().abs().sum(axis=1).mean()*10)\n",
    "    \n",
    "    mean_loss_val = np.mean(losses_val)\n",
    "    losses_val_history.append(mean_loss_val)\n",
    "    mean_buy_sell_expenses = np.mean(mean_buy_sell_expenses)\n",
    "    \n",
    "    print(mean_loss_val)\n",
    "#     print(\"losses_epoch:\", losses_epoch)\n",
    "#     print(\"losses_val_history:\", losses_val_history)\n",
    "    \n",
    "    mean_L1 = np.mean(L1s)\n",
    "    mean_L2 = np.mean(L2s)\n",
    "    mean_profit = np.mean(profits)\n",
    "    mean_profit_with_expenses = mean_profit-0.001*mean_buy_sell_expenses\n",
    "\n",
    "    mean_epoch_loss = np.mean(losses_epoch)\n",
    "    print(f\"epoch = {epoch}; mean_epoch_loss = {mean_epoch_loss:.3f}; val_loss = {mean_loss_val:.3f}; dist_from_200_loss = {mean_L1:.3f}; betas_loss = {mean_L2:.3f}; mean_buy_sell_expenses = {mean_buy_sell_expenses:.3f}; mean_profit = {mean_profit:.6f}; mean_profit_with_expenses = {mean_profit_with_expenses:.6f};\")\n",
    "    scheduler.step(mean_loss_val)\n",
    "\n",
    "    if mean_loss_val<=min(losses_val_history):\n",
    "        print(\"saving model.\")\n",
    "        save_path = f\"./model_vindish_epoch_{epoch}_train_loss_{int(mean_epoch_loss)}_val_loss{int(mean_loss_val)}.pth.tar\"\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    else:\n",
    "        print(\"not saving model.\")\n",
    "    \n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    writer.add_scalar(\"mean_epoch_loss\", mean_epoch_loss, epoch)\n",
    "    writer.add_scalar(\"mean_loss_val\", mean_loss_val, epoch)\n",
    "    writer.add_scalar(\"dist_from_200_loss\", mean_L1, epoch)\n",
    "    writer.add_scalar(\"betas_loss\", mean_L2, epoch)\n",
    "    writer.add_scalar(\"mean_profit\", mean_profit, epoch)\n",
    "    writer.add_scalar(\"mean_profit_with_expenses\", mean_profit_with_expenses, epoch)\n",
    "    writer.add_scalar(\"mean_buy_sell_expenses\", mean_buy_sell_expenses, epoch)\n",
    "    writer.add_scalar(\"lr\", lr, epoch)\n",
    "    \n",
    "    losses_epoch = []; L1s = [];  L2s = []; profits = []; mean_buy_sell_expenses=[]\n",
    "\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(1+mean_profit)**(6*8*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = X_test[:2]\n",
    "# xx\n",
    "xx[:, -1, 1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = y_test[:2]\n",
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = model.to(\"cpu\")(xx)\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas.abs().cpu().detach().numpy().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas@betas.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_profit(yy, xx[:, -1, 1:6], alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change a, b during epochs so that profit will also be tuned during initial steps of optimization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "w_fc3 = list(model.fc3.parameters())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "w_fc3[0] = torch.zeros_like(w_fc3[0])\n",
    "w_fc3[1] = torch.zeros_like(w_fc3[1])\n",
    "w_fc3[1][0] = 200."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list(model.fc3.parameters())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.fc3.weight.data = (torch.zeros_like(model.fc3.weight))\n",
    "model.fc3.bias.data = torch.zeros_like(model.fc3.bias)\n",
    "model.fc3.bias[0] = 200."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x = X[:64, :, :]\n",
    "y = y[:64, :]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "calc_loss(model(x), betas, x.to(device), y.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1+0.08/200)**(6*9*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1+0.05/200)**(6*9*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### testing\n",
    "\n",
    "\n",
    "model = Model()\n",
    "# PATH = save_path\n",
    "PATH = \"model_vindish_epoch_18_train_loss_-280_val_loss107.pth.tar\"\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "profits = []\n",
    "costs = []\n",
    "trans_cost = 0.01\n",
    "alphas_test = []\n",
    "\n",
    "x, y = next(iter(test_dl))\n",
    "x, y = x.to(device), y.to(device)\n",
    "alphas_test.append(model(x).type(torch.Tensor).to(device))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for x, y in tqdm(test_dl): # batch size is 1 for testing\n",
    "    i += 1\n",
    "#     if i == 10:\n",
    "#         break\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    alphas_test.append(model(x).type(torch.Tensor).to(device))\n",
    "#         loss_val = calc_loss(alphas_val, betas, x_val, y_val)\n",
    "#         losses_val.append(loss_val.cpu().detach().numpy())\n",
    "#         L1s.append(get_dist_from_200(alphas_val).cpu().detach().numpy().mean())\n",
    "#         L2s.append(get_hedging_score(alphas_val, betas).cpu().detach().numpy().mean())\n",
    "    profits.append(get_profit(y, x[:, -1, 1:6], alphas_test[-1]).cpu().detach().numpy().mean())\n",
    "    costs.append(np.abs(alphas_test[-1].cpu().detach().numpy()-alphas_test[-2].cpu().detach().numpy()).sum()*trans_cost)\n",
    "\n",
    "print(\"Total profit including costs: {:.4f}\".format(sum(profits+costs)))\n",
    "pd.DataFrame({\"profits\":profits, \"costs\":costs})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha_50 = pd.Series(alphas_test)[51]#.apply(lambda x: x.cpu().detach().numpy()[0])[50]\n",
    "# alpha_50\n",
    "# len(alphas_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_test[50:51]\n",
    "y = y_test[50:51]\n",
    "x, y = x.to(device), y.to(device)\n",
    "get_profit(y, x[:, -1, 1:6], alpha_50).cpu().detach().numpy().mean()\n",
    "# len(profits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_test = model(x0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6*24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl1",
   "language": "python",
   "name": "rl1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
