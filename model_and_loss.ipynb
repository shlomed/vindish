{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!pip install tb-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor, nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import dill\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader_pytorch_class import VindishDataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset = VindishDataset(seed=None)\n",
    "dataloader = DataLoader(dataset, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlphaExtractor, self).__init__()\n",
    "        self.fc1 = nn.Linear(17, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc1(x[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaExtractorFull(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlphaExtractorFull, self).__init__()\n",
    "        self.alpha_extractor = AlphaExtractor()\n",
    "        self.fc2 = nn.Linear(5, 5) # sanity check for no_grad\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        with torch.no_grad():\n",
    "            alpha_tm2 = self.alpha_extractor(x1)\n",
    "            alpha_tm2 = self.fc2(alpha_tm2) # sanity check for no_grad\n",
    "        \n",
    "        alpha_tm1 = self.alpha_extractor(x2)\n",
    "        \n",
    "        return alpha_tm2, alpha_tm1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "alpha_extractor_full = AlphaExtractorFull()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_0_tm2, x_1_tm1, y_tm1, y_t = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_0_tm2.shape, x_1_tm1.shape, y_tm1.shape, y_t.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "alpha_tm2, alpha_tm1 = alpha_extractor_full(x_0_tm2, x_1_tm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(alpha_tm2, alpha_tm1, y_tm1, y_t, cost_per_unit=0.1, desired_n_units=200.,\n",
    "            w_profit=1., w_costs=1., w_hedge=1., w_tot_units=1.,\n",
    "            betas = torch.tensor(np.array([0.62, 0.43, 0.32, 0.25, 0.21])).to(device) ):\n",
    "\n",
    "    L_profit = (alpha_tm1.mul(y_t - y_tm1)).sum(axis=1)\n",
    "    L_costs = torch.abs(alpha_tm2-alpha_tm1).sum(axis=1)*cost_per_unit\n",
    "    L_hedge = ((alpha_tm1*betas)**2).sum(dim=1)\n",
    "    L_tot_units = (torch.abs(alpha_tm1).sum(axis=1)-desired_n_units)**2\n",
    "\n",
    "#     print(L_profit.shape, L_costs.shape, L_hedge.shape, L_tot_units.shape)\n",
    "    res = {}\n",
    "    res[\"tot_loss\"] = torch.mean( w_profit*L_profit + w_costs*L_costs + w_hedge*L_hedge + w_tot_units*L_tot_units ) \n",
    "    res[\"L_profit\"] = L_profit\n",
    "    res[\"L_costs\"] = L_costs\n",
    "    res[\"L_hedge\"] = L_hedge\n",
    "    res[\"L_tot_units\"] = L_tot_units\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "alpha_extractor_full.zero_grad()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loss = get_loss(alpha_tm2, alpha_tm1, y_tm1, y_t)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "optimizer = torch.optim.Adam(alpha_extractor_full.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class full_object():\n",
    "    def __init__(self, HPs):\n",
    "        self.HPs = HPs\n",
    "        self.alpha_extractor = AlphaExtractor()\n",
    "        self._train_extractor = AlphaExtractorFull(self.alpha_extractor)\n",
    "        \n",
    "        self.loss = self._get_loss\n",
    "        self.optim = 1\n",
    "        \n",
    "        self.dataloader = 23\n",
    "        self.schdualer = 15\n",
    "        \n",
    "        \n",
    "    def train():\n",
    "        pass\n",
    "    \n",
    "    def predict(x):\n",
    "        return self.alpha_extractor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(alpha_tm2, alpha_tm1, y_tm1, y_t, alpha_extractor_full, optimizer):\n",
    "    alpha_extractor_full.train()\n",
    "    optimizer.zero_grad()\n",
    "    losses = get_loss(alpha_tm2, alpha_tm1, y_tm1, y_t)\n",
    "    losses[\"tot_loss\"].backward()#retain_graph=True)\n",
    "    optimizer.step()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_end_of_batch_str(epoch_num, batch_num, running_loss, losses):\n",
    "    str_epoch = f\"epoch: {epoch_num}/{epochs}\"\n",
    "    str_batch = f\"batch: {batch_num}/{len(dataloader)}\"\n",
    "    str_loss = f\"total_loss: {running_loss:.2f}\"\n",
    "    str_L_profit = f\"L_profit: {losses['L_profit'].cpu().mean().detach().numpy():.2f}\"\n",
    "    str_L_tot_units = f\"L_tot_units: {losses['L_tot_units'].cpu().mean().detach().numpy():.2f}\"\n",
    "    str_L_hedge = f\"L_hedge: {losses['L_hedge'].cpu().mean().detach().numpy():.2f}\"\n",
    "    str_L_costs = f\"L_costs: {losses['L_costs'].cpu().mean().detach().numpy():.2f}\"\n",
    "\n",
    "    output_str = \", \".join([str_epoch, str_batch, str_loss, str_L_profit, \n",
    "                     str_L_tot_units, str_L_hedge, str_L_costs])\n",
    "\n",
    "    print(\"\\r\" + end_of_batch_str(epoch_num, batch_num, running_loss, losses), flush=True, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 4000\n",
    "alpha_extractor_full = AlphaExtractorFull()\n",
    "alpha_extractor_full.to(device)\n",
    "optimizer = torch.optim.Adam(alpha_extractor_full.parameters(), lr=0.01)\n",
    "dataset = VindishDataset(seed=None)\n",
    "\n",
    "for epoch_num in range(epochs):\n",
    "    print()\n",
    "    running_loss = 0\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    for batch_num, (x_0_tm2, x_1_tm1, y_tm1, y_t) in enumerate(dataloader):\n",
    "        x_0_tm2, x_1_tm1, y_tm1, y_t = x_0_tm2.to(device), x_1_tm1.to(device), y_tm1.to(device), y_t.to(device)\n",
    "        alpha_tm2, alpha_tm1 = alpha_extractor_full(x_0_tm2, x_1_tm1)\n",
    "        losses = train_step(alpha_tm2, alpha_tm1, y_tm1, y_t, alpha_extractor_full, optimizer)\n",
    "        running_loss = (batch_num*running_loss + losses[\"tot_loss\"]) / (batch_num+1)\n",
    "        print_end_of_batch_str(epoch_num, batch_num, running_loss, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, alpha_extractor_full, dataset_train, dataset_eval, models_path=\"models/\", device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        self.device = device\n",
    "        self.alpha_extractor_full = alpha_extractor_full.to(self.device)\n",
    "        self.models_path = models_path if models_path.endswith(\"/\") else models_path + \"/\"\n",
    "        self.dataset_train = dataset_train\n",
    "        self.dataset_eval = dataset_eval\n",
    "        \n",
    "        self._set_general_params()\n",
    "        self._set_loss_params()\n",
    "        self._set_optimizer()\n",
    "        self._set_lr_schedualer()\n",
    "        \n",
    "    def _set_general_params(self, batch_size=128):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def _set_loss_params(self, \n",
    "                        cost_per_unit=0.1, \n",
    "                        desired_n_units=200.,\n",
    "                        w_profit=1., w_costs=1., w_hedge=1., w_tot_units=1.,\n",
    "                        betas = torch.tensor(np.array([0.62, 0.43, 0.32, 0.25, 0.21])) ):\n",
    "        \n",
    "        self.cost_per_unit = cost_per_unit\n",
    "        self.desired_n_units = desired_n_units\n",
    "        self.w_profit = w_profit\n",
    "        self.w_costs = w_costs\n",
    "        self.w_hedge = w_hedge\n",
    "        self.w_tot_units = w_tot_units\n",
    "        self.betas = betas.to(self.device)\n",
    "\n",
    "    def _set_optimizer(self):\n",
    "        self.optimizer = torch.optim.Adam(self.alpha_extractor_full.parameters(), lr=0.01)\n",
    "        \n",
    "    def _set_lr_schedualer(self, patience=5):\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=1/np.sqrt(10), patience=patience)\n",
    "        \n",
    "    def save(self, chkpnt_str=\"\"):\n",
    "        os.makedirs(models_path, exist_ok=True)\n",
    "        torch.save(self.alpha_extractor_full.state_dict(), self.models_path + f\"model_{chkpnt_str}.pt\")\n",
    "        torch.save(self.optimizer.state_dict(), self.models_path + f\"optimizer_{chkpnt_str}.pt\")\n",
    "        \n",
    "    def load_state_dict(self, model_path, optimizer_path=None):\n",
    "        self.alpha_extractor_full.load_state_dict(torch.load(model_path))\n",
    "        if optimizer_path is not None:\n",
    "            self.optimizer.load_state_dict(torch.load(optimizer_path))        \n",
    "    \n",
    "    def get_loss(self, alpha_tm2, alpha_tm1, y_tm1, y_t):\n",
    "\n",
    "        L_profit = (alpha_tm1.mul(y_t - y_tm1)).sum(axis=1)\n",
    "        L_costs = torch.abs(alpha_tm2-alpha_tm1).sum(axis=1)*self.cost_per_unit\n",
    "        L_hedge = ((alpha_tm1*self.betas)**2).sum(dim=1)\n",
    "        L_tot_units = (torch.abs(alpha_tm1).sum(axis=1)-self.desired_n_units)**2\n",
    "\n",
    "    #     print(L_profit.shape, L_costs.shape, L_hedge.shape, L_tot_units.shape)\n",
    "        res = {}\n",
    "        res[\"tot_loss\"] = torch.mean( self.w_profit*L_profit + self.w_costs*L_costs + self.w_hedge*L_hedge + self.w_tot_units*L_tot_units ) \n",
    "        res[\"L_profit\"] = L_profit\n",
    "        res[\"L_costs\"] = L_costs\n",
    "        res[\"L_hedge\"] = L_hedge\n",
    "        res[\"L_tot_units\"] = L_tot_units\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def _train_step(self, alpha_tm2, alpha_tm1, y_tm1, y_t):\n",
    "        self.alpha_extractor_full.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        losses = self.get_loss(alpha_tm2, alpha_tm1, y_tm1, y_t)\n",
    "        losses[\"tot_loss\"].backward()#retain_graph=True)\n",
    "        self.optimizer.step()\n",
    "        return losses\n",
    "    \n",
    "    def print_end_of_batch_str(self, epochs, dataloader, epoch_num, batch_num, running_loss, losses):\n",
    "        str_epoch = f\"epoch: {epoch_num}/{epochs}\"\n",
    "        str_batch = f\"batch: {batch_num}/{len(dataloader)}\"\n",
    "        str_loss = f\"total_loss: {running_loss:.2f}\"\n",
    "        str_L_profit = f\"L_profit: {losses['L_profit'].cpu().mean().detach().numpy():.2f}\"\n",
    "        str_L_tot_units = f\"L_tot_units: {losses['L_tot_units'].cpu().mean().detach().numpy():.2f}\"\n",
    "        str_L_hedge = f\"L_hedge: {losses['L_hedge'].cpu().mean().detach().numpy():.2f}\"\n",
    "        str_L_costs = f\"L_costs: {losses['L_costs'].cpu().mean().detach().numpy():.2f}\"\n",
    "\n",
    "        output_str = \", \".join([str_epoch, str_batch, str_loss, str_L_profit, \n",
    "                         str_L_tot_units, str_L_hedge, str_L_costs])\n",
    "\n",
    "        print(\"\\r\" + output_str, flush=True, end=\"\")\n",
    "\n",
    "    def train(self, epochs=1):\n",
    "        for epoch_num in range(epochs):\n",
    "            print()\n",
    "            running_loss = 0\n",
    "            dataloader = DataLoader(self.dataset_train, batch_size=self.batch_size)\n",
    "            \n",
    "            self.alpha_extractor_full.train()\n",
    "            for batch_num, (x_0_tm2, x_1_tm1, y_tm1, y_t) in enumerate(dataloader):\n",
    "                x_0_tm2, x_1_tm1, y_tm1, y_t = x_0_tm2.to(device), x_1_tm1.to(device), y_tm1.to(device), y_t.to(device)\n",
    "                alpha_tm2, alpha_tm1 = self.alpha_extractor_full(x_0_tm2, x_1_tm1)\n",
    "                losses = self._train_step(alpha_tm2, alpha_tm1, y_tm1, y_t)\n",
    "                running_loss = (batch_num*running_loss + losses[\"tot_loss\"]) / (batch_num+1)\n",
    "                self.print_end_of_batch_str(epochs, dataloader, epoch_num, batch_num, running_loss, losses)\n",
    "            \n",
    "#             # validation & lr-reduction\n",
    "#             self.alpha_extractor_full.eval()\n",
    "#             for ... # eval loop\n",
    "            \n",
    "#             val_loss = ...\n",
    "#             self.scheduler.step(val_loss)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = VindishDataset(seed=None)\n",
    "dataset_eval = VindishDataset(seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_extractor_full = AlphaExtractorFull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(alpha_extractor_full, dataset_train, dataset_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
